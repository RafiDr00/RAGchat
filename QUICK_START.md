# RAGchat - Quick Start Guide

## ğŸ¯ What You Have

A **production-ready** RAG (Retrieval-Augmented Generation) system with:

### âœ… Supported File Types (14+ formats)
- **Documents**: PDF (with OCR), TXT, MD, CSV, JSON, XML, HTML
- **Images**: PNG, JPG, JPEG, TIFF, BMP, GIF, WEBP (with OCR)

### âœ… Key Features
1. **RAG Toggle**: Switch between grounded (using your documents) and raw LLM modes
2. **Hybrid Search**: Semantic + keyword retrieval for best results
3. **Multi-format Processing**: Automatic detection and parsing
4. **Real-time UI**: Upload progress, chunk visualization, relevance scores
5. **OCR Support**: Extract text from scanned PDFs and images

---

## ğŸš€ How to Start

### Option 1: Use Startup Scripts (Easiest)

**Windows**:
```bash
# Terminal 1 - Backend
start-backend.bat

# Terminal 2 - Frontend  
start-frontend.bat
```

### Option 2: Manual Start

**Backend**:
```bash
cd backend
venv\Scripts\activate
python -m uvicorn main:app --reload --port 8000
```

**Frontend**:
```bash
cd frontend
npm run dev
```

---

## ğŸ“ Access Points

- **Frontend**: http://localhost:3001
- **Backend API**: http://localhost:8000
- **API Docs**: http://localhost:8000/docs

---

## ğŸ® How to Use

### 1. Upload Documents
- Click the upload button (bottom center)
- Select any supported file type
- Watch the scanning progress
- See chunk count increase

### 2. Toggle RAG Mode
- **GROUNDED** (green): Uses your uploaded documents
- **RAW** (gray): Direct LLM without documents

### 3. Ask Questions
- Type your question in the input field
- Press Enter or click the emerald chevron
- See both RAG and LLM answers side-by-side

### 4. View Relevance
- Each chunk shows a match score (0-100)
- Higher score = more relevant to your question
- Hover over chunks to see full text

---

## ğŸ”§ Configuration

### Required (for LLM functionality)
Create `backend/.env`:
```env
OPENAI_API_KEY=sk-your-key-here
OPENAI_MODEL=gpt-3.5-turbo
```

### Optional (for local LLM)
```env
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama2
```

---

## ğŸ› Troubleshooting

### Backend won't start
- Check if Python venv is created: `cd backend && python -m venv venv`
- Install dependencies: `pip install -r requirements.txt`
- Verify Python version: `python --version` (need 3.8+)

### Frontend won't start
- Install dependencies: `cd frontend && npm install`
- Clear cache: `npm cache clean --force`
- Delete `node_modules` and reinstall

### OCR not working
- Windows: Install Tesseract from https://github.com/UB-Mannheim/tesseract/wiki
- Add to PATH: `C:\Program Files\Tesseract-OCR`
- For scanned PDFs, also install Poppler

### File upload fails
- Check file size (max 20MB)
- Verify file extension is supported
- Check backend logs for errors

---

## ğŸ“Š System Architecture

```
Frontend (Next.js) â†’ API Proxy â†’ Backend (FastAPI)
                                    â†“
                            RAG Service
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          Document Processor    Embedder
                 â†“                     â†“
          Text Extraction      Sentence Transformers
                 â†“                     â†“
              Chunking            Vector Storage
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      Hybrid Retrieval
                            â†“
                      LLM Service (OpenAI/Ollama)
```

---

## ğŸ¨ Features Breakdown

### File Upload & Processing
- Multi-format detection
- Automatic text extraction
- OCR fallback for images/scanned PDFs
- Progress tracking
- Error handling

### RAG Pipeline
1. **Ingestion**: File â†’ Text â†’ Embeddings â†’ Chunks
2. **Indexing**: Store in memory with vectors
3. **Retrieval**: Query â†’ Hybrid search â†’ Top-5 chunks
4. **Generation**: Chunks + Question â†’ LLM â†’ Answer

### Frontend UI
- **Volumetric Space**: Animated starfield background
- **Magnetic Cursor**: Interactive emerald cursor
- **Glass Morphism**: Frosted glass UI components
- **Relevance Rings**: Visual match scores
- **Neural Shimmer**: Loading animations

---

## ğŸ“ˆ Performance

- **Upload**: ~1-3s per document (depending on size)
- **Query**: ~500ms-2s (with LLM)
- **Chunk Size**: 512 characters with 50 char overlap
- **Retrieval**: Top-5 most relevant chunks

---

## ğŸ” Security Notes

- **CORS**: Enabled for localhost (disable in production)
- **File Validation**: MIME type + extension checking
- **Path Traversal**: Sanitized filenames
- **Size Limits**: 20MB max upload
- **API Keys**: Stored in .env (gitignored)

---

## ğŸ“ Project Structure

```
RAGchat/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                 # FastAPI server
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ .env                    # Config (gitignored)
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ documents/          # Pre-loaded files
â”‚   â”‚   â””â”€â”€ uploads/            # User uploads
â”‚   â””â”€â”€ rag/
â”‚       â”œâ”€â”€ service.py          # RAG orchestration
â”‚       â”œâ”€â”€ document_processor.py  # Multi-format parsing
â”‚       â”œâ”€â”€ embeddings.py       # Sentence transformers
â”‚       â”œâ”€â”€ retrieval.py        # Hybrid search
â”‚       â””â”€â”€ llm_service.py      # OpenAI/Ollama
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/app/
â”‚   â”‚   â”œâ”€â”€ page.tsx            # Main UI
â”‚   â”‚   â”œâ”€â”€ layout.tsx          # Layout + metadata
â”‚   â”‚   â””â”€â”€ globals.css         # Design system
â”‚   â”œâ”€â”€ next.config.js          # API proxy config
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ IMPLEMENTATION_STATUS.md    # Full system audit
â”œâ”€â”€ start-backend.bat           # Windows backend starter
â””â”€â”€ start-frontend.bat          # Windows frontend starter
```

---

## âœ… Verification Checklist

Before deploying, verify:
- [ ] Backend starts without errors
- [ ] Frontend connects to backend
- [ ] File upload works for all types
- [ ] RAG toggle changes answer behavior
- [ ] Chunk count updates after upload
- [ ] Relevance scores display correctly
- [ ] LLM responds (API key configured)
- [ ] OCR works (if Tesseract installed)

---

## ğŸ’¡ Tips & Best Practices

1. **Document Naming**: Use descriptive filenames for better organization
2. **Chunk Size**: 512 chars is optimal for most use cases
3. **Top-K**: 5 chunks balances relevance vs context
4. **RAG Toggle**: Use GROUNDED for factual queries, RAW for creative
5. **File Formats**: Text-based PDFs are faster than scanned
6. **Query Phrasing**: Clear, specific questions get better results

---

## ğŸ“ Example Workflows

### Research Assistant
1. Upload research papers (PDFs)
2. Ask: "What are the main findings?"
3. Compare RAG vs LLM answers
4. Export relevant chunks

### Documentation Search
1. Upload project docs (MD, TXT)
2. Toggle GROUNDED mode
3. Ask how-to questions
4. Get cited, grounded answers

### Invoice Processing
1. Upload invoices (PDF, images)
2. OCR extracts text automatically
3. Query: "What is the total amount?"
4. Get structured data

---

## ğŸ“ Support & Resources

- **System Status**: Check `IMPLEMENTATION_STATUS.md`
- **API Docs**: http://localhost:8000/docs (when backend running)
- **Logs**: Backend terminal shows processing details
- **Health Check**: http://localhost:8000/health

---

## ğŸ‰ You're Ready!

Your RAGchat system is **fully operational**. Just start the backend and you're good to go!

```bash
# Start both servers, then visit:
http://localhost:3001
```

Happy querying! ğŸš€
